{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidj4tech/sacred-brain/blob/main/llama_cpp_python_ngrok.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggitswn7p22A"
      },
      "source": [
        "# DeepSeek via llama-cpp-python (CUDA) + ngrok ‚Äî Colab (updated)\n",
        "\n",
        "This notebook:\n",
        "1) Verifies you have a GPU (T4 etc.)\n",
        "2) Installs **llama-cpp-python with CUDA + server extras** (no compiling)\n",
        "3) Downloads the DeepSeek GGUF model\n",
        "4) Starts an **OpenAI-compatible** server on an available port (default **8081** to avoid conflicts)\n",
        "5) Exposes it via **ngrok**\n",
        "\n",
        "## Colab Secrets\n",
        "- `NGROK_AUTH_TOKEN` (required)\n",
        "- `HF_TOKEN` (optional; only for gated/private HF repos)\n"
      ],
      "id": "Ggitswn7p22A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgRgUnDxp22L"
      },
      "outputs": [],
      "source": [
        "# 1) Confirm GPU + CUDA\n",
        "!nvidia-smi -L\n",
        "!nvcc --version || true\n",
        "!free -h\n"
      ],
      "id": "rgRgUnDxp22L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO6mYt3ep22P"
      },
      "outputs": [],
      "source": [
        "# 2) Install llama-cpp-python CUDA wheel + server extras\n",
        "# CUDA drivers in Colab may be 12.5; cu121 wheels generally work fine.\n",
        "\n",
        "!pip -q uninstall -y llama-cpp-python || true\n",
        "!pip -q install \"llama-cpp-python[server]\" --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
        "\n",
        "# Basic import sanity check\n",
        "import llama_cpp\n",
        "print('llama_cpp import OK')\n"
      ],
      "id": "QO6mYt3ep22P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm6xLp7cp22Q"
      },
      "outputs": [],
      "source": [
        "# 3) (Optional) Hugging Face login (only needed for gated/private models)\n",
        "!pip -q install huggingface_hub\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "if HF_TOKEN:\n",
        "    login(token=HF_TOKEN)\n",
        "    print('‚úÖ Logged into Hugging Face')\n",
        "else:\n",
        "    print('‚ÑπÔ∏è No HF_TOKEN set (fine for public repos)')\n"
      ],
      "id": "gm6xLp7cp22Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU9AdgJZp22R"
      },
      "outputs": [],
      "source": [
        "# 4) Download model GGUF\n",
        "!mkdir -p /content/models\n",
        "\n",
        "MODEL_URL = (\n",
        "  'https://huggingface.co/Triangle104/DeepSeek-R1-Distill-Qwen-7B-uncensored-Q5_K_S-GGUF/'\n",
        "  'resolve/main/deepseek-r1-distill-qwen-7b-uncensored-q5_k_s.gguf'\n",
        ")\n",
        "MODEL_PATH = '/content/models/deepseek.gguf'\n",
        "\n",
        "!wget -q --show-progress -O \"$MODEL_PATH\" \"$MODEL_URL\"\n",
        "!ls -lh /content/models\n"
      ],
      "id": "YU9AdgJZp22R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzGWr9dRp22S"
      },
      "outputs": [],
      "source": [
        "# 5) Free the port (default 8081) and start server in BACKGROUND\n",
        "import subprocess, time\n",
        "import requests\n",
        "\n",
        "PORT = 8081\n",
        "HOST = '127.0.0.1'\n",
        "LOG = '/content/llama_server.log'\n",
        "\n",
        "# Kill anything listening on our port + any previous llama_cpp.server\n",
        "subprocess.run(f\"fuser -k {PORT}/tcp\", shell=True)\n",
        "subprocess.run(\"pkill -f 'python -m llama_cpp.server'\", shell=True)\n",
        "time.sleep(1)\n",
        "\n",
        "cmd = [\n",
        "    'python', '-m', 'llama_cpp.server',\n",
        "    '--model', '/content/models/deepseek.gguf',\n",
        "    '--host', HOST,\n",
        "    '--port', str(PORT),\n",
        "    '--n_gpu_layers', '99',\n",
        "    '--n_ctx', '2048',\n",
        "]\n",
        "\n",
        "with open(LOG, 'w') as f:\n",
        "    p = subprocess.Popen(cmd, stdout=f, stderr=subprocess.STDOUT)\n",
        "\n",
        "print('‚úÖ Server starting (PID):', p.pid)\n",
        "print('Log:', LOG)\n",
        "\n",
        "health_url = f'http://{HOST}:{PORT}/health'\n",
        "for i in range(120):\n",
        "    try:\n",
        "        r = requests.get(health_url, timeout=1)\n",
        "        if r.status_code == 200:\n",
        "            print('‚úÖ Server healthy:', health_url)\n",
        "            break\n",
        "    except Exception:\n",
        "        pass\n",
        "    time.sleep(1)\n",
        "else:\n",
        "    print('‚ö†Ô∏è Server did not become healthy in time. Tail log:')\n",
        "    subprocess.run('tail -n 200 /content/llama_server.log', shell=True)\n",
        "\n",
        "print('\\nGPU snapshot:')\n",
        "subprocess.run('nvidia-smi | head -n 30', shell=True)\n"
      ],
      "id": "hzGWr9dRp22S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5ejYoxop22U"
      },
      "outputs": [],
      "source": [
        "# 6) Expose via ngrok (matches PORT above)\n",
        "!pip -q install pyngrok\n",
        "\n",
        "from google.colab import userdata\n",
        "from pyngrok import ngrok\n",
        "\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "if not NGROK_AUTH_TOKEN:\n",
        "    raise RuntimeError('Set NGROK_AUTH_TOKEN in Colab Secrets as NGROK_AUTH_TOKEN')\n",
        "\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "public = ngrok.connect(8081, 'http')\n",
        "\n",
        "print('üåç Public URL:', public.public_url)\n",
        "print('OpenAI base:', public.public_url + '/v1')\n"
      ],
      "id": "f5ejYoxop22U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfXD-tl2p22V"
      },
      "outputs": [],
      "source": [
        "# 7) Test locally (OpenAI-style)\n",
        "import requests\n",
        "\n",
        "base = 'http://127.0.0.1:8081/v1'\n",
        "payload = {\n",
        "  'model': 'local',\n",
        "  'messages': [\n",
        "    {'role': 'user', 'content': 'Say hi in one sentence, then give 3 Linux backup tips.'}\n",
        "  ],\n",
        "  'temperature': 0.6\n",
        "}\n",
        "\n",
        "r = requests.post(base + '/chat/completions', json=payload, timeout=300)\n",
        "print('HTTP', r.status_code)\n",
        "print(r.json()['choices'][0]['message']['content'])\n"
      ],
      "id": "IfXD-tl2p22V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhuG8IRpp22W"
      },
      "outputs": [],
      "source": [
        "# 8) Useful: show what's listening + tail logs\n",
        "!lsof -iTCP:8081 -sTCP:LISTEN -n -P || true\n",
        "!tail -n 200 /content/llama_server.log\n"
      ],
      "id": "jhuG8IRpp22W"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}